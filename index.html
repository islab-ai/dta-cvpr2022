<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
    google.load("jquery", "1.3.2");
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.0.0/jquery.min.js"></script>

<!-- jQuery Modal -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-modal/0.9.1/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery-modal/0.9.1/jquery.modal.min.css" />

<div class="topnav" id="myTopnav">
    <a href="http://infosec.pusan.ac.kr/"><img width="100%" src="assets/infosec_logo.png"></a>
    <a href="https://www.smartm2m.co.kr/"><img width="100%" src="assets/smartm2m_blk_logo.png"></a>
    <a href="https://add.re.kr/"><img width="100%" src="assets/add_logo.png"></a>
</div>

<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>

<head>
    <title>DTA: Physical Camouflage Attacks using Differentiable Transformation Network</title>
    <meta property="og:description"
        content="DTA: Physical Camouflage Attacks using Differentiable Transformation Network" />
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

    <link rel="stylesheet" href="style.css">

</head>


<body>
    <div class="container">
        
        <div id="avs" class="modal">
            <p>Available soon...</p>
        </div>

        <div class="paper-title">
            <h1>DTA: Physical Camouflage Attacks using Differentiable Transformation Network</h1>
            <h2>CVPR 2022 - Accepted Paper</h2>
        </div>

        <div id="authors">
            <div class="author-row">
                <div class="col-3 text-center"><a href="https://www.linkedin.com/in/naufal-suryanto/">Naufal
                        Suryanto</a><sup>1</sup></div>
                <div class="col-3 text-center"><a href="https://scholar.google.co.kr/citations?user=DXb797cAAAAJ">Yongsu
                        Kim</a><sup>1,2</sup></div>
                <div class="col-3 text-center"><a href="https://scholar.google.co.kr/citations?user=GeQi_D4AAAAJ">
                        Hyoeun Kang</a><sup>1</sup></div>
                <div class="col-3 text-center"><a href="https://scholar.google.co.id/citations?user=S8lwCEUAAAAJ">
                        Harashta Tatimma Larasati</a><sup>1</sup></div>
                <div class="col-3 text-center"><a href="https://scholar.google.com/citations?user=dFZRNOEAAAAJ">
                        Youngyeo Yun</a><sup>1</sup>
                </div>
                <div class="col-3 text-center"><a href="https://scholar.google.com/citations?user=UptzPYsAAAAJ">
                        Thi-Thu-Huong Le</a><sup>1</sup></div>
                <div class="col-3 text-center"><a href="https://scholar.google.co.kr/citations?user=mDxJj2AAAAAJ">Hunmin
                        Yang</a><sup>3</sup></div>
                <div class="col-3 text-center"><a href="https://ieeexplore.ieee.org/author/37088566336">Se-Yoon
                        Oh</a><sup>3</sup></div>
                <div class="col-3 text-center"><a href="https://ieeexplore.ieee.org/author/37082911200">Howon
                        Kim</a><sup>1,2</sup></div>
            </div>

            <div class="affil-row">
                <div class="col-3 text-center"><sup>1</sup>Pusan National University</a></div>
                <div class="col-3 text-center"><sup>2</sup>SmartM2M</div>
                <div class="col-3 text-center"><sup>3</sup>Agency for Defense Development</div>
            </div>
            <!-- <div class="affil-row">
            <div class="venue text-center"><b></b></div>
        </div> -->

            <div style="clear: both">
                <div class="paper-btn-parent">
                    <a class="supp-btn" href="#avs" rel="modal:open">
                        <span class="material-icons"> description </span>
                        Paper
                    </a>
                    <a class="supp-btn" href="#avs" rel="modal:open">
                        <span class="material-icons"> description </span>
                        BibTeX
                    </a>
                </div>
            </div>
        </div>

        <section id="teaser">
            <figure style="width: 100%;">
                <a href="assets/DTA_attack_pipeline.png">
                    <img width="100%" src="assets/DTA_attack_pipeline.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    <b>Differentiable Transformation Attack (DTA) </b> is our proposed framework for generating a robust
                    physical adversarial pattern on a target object to camouflage it against object detection models
                    under a wide range of transformations.
                    Our framework uses legacy photo-realistic renderers for simulating physical-world transformations
                    and employs our novel <b>Differentiable Transformation Network (DTN)</b> to enable texture
                    differentiability. Our framework produces a robust adversarial texture as a repeated pattern
                    applicable and transferable even in the real world.
                    <!-- DTN is trained to learn the expected transformation of a rendered object when the texture is
                    changed while retaining the target object's original properties. -->
                </p>
            </figure>
        </section>

        <section id="abstract" />
        <h2>Abstract</h2>
        <hr>
        <p>
            To perform adversarial attacks in the physical world, many studies have proposed adversarial camouflage, a
            method to hide a target object by applying camouflage patterns on 3D object surfaces.
            For obtaining optimal physical adversarial camouflage, previous studies have utilized the so-called neural
            renderer, as it supports differentiability. However, existing neural renderers cannot fully represent
            various real-world transformations due to a lack of control of scene parameters compared to the legacy
            photo-realistic renderers. In this paper, we propose the Differentiable Transformation Attack (DTA), a
            framework for generating a robust physical adversarial pattern on a target object to camouflage it against
            object detection models with a wide range of transformations. It utilizes our novel Differentiable
            Transformation Network (DTN), which learns the expected transformation of a rendered object when the texture
            is changed while preserving the original properties of the target object. Using our attack framework, an
            adversary can gain both the advantages of the legacy photo-realistic renderers including various
            physical-world transformations and the benefit of white-box access by offering differentiability. Our
            experiments show that our camouflaged 3D vehicles can successfully evade state-of-the-art object
            detection models in the photo-realistic environment (i.e., <a href="https://carla.org/">CARLA</a> on <a
                href="https://www.unrealengine.com/en-US/">Unreal Engine</a>). Furthermore, our
            demonstration on a scaled Tesla Model 3 proves the applicability and transferability of our method to the
            real world.
        </p>

        <table>
            <thead>
                <tr>
                    <th align="center">Photo-Realistic Simulation Demo</th>
                    <th align="center">Real World Demo</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td align="center">
                        <video class="centered" width="95%" muted loop autoplay>
                            <source src="assets/video/dta_photo_realistic_demo.webm" type="video/webm">
                            Your browser does not support the video tag.
                        </video>
                    </td>
                    <td align="center">
                        <video class="centered" width="95%" muted loop autoplay>
                            <source src="assets/video/dta_real_world_demo.webm" type="video/webm">
                            Your browser does not support the video tag.
                        </video>
                    </td>
                </tr>
            </tbody>
        </table>

        <!-- <div class="flex-row">
            <figure style="width: 50%;">
                <a href="assets/video/tesla_normal_detect.webp">
                    <img width="98%" src="assets/video/tesla_normal_detect.webp">
                </a>
            </figure>
            <figure style="width: 50%;">
                <a href="assets/video/tesla_attack_detect.webp">
                    <img width="98%" src="assets/video/tesla_attack_detect.webp">
                </a>
            </figure>
        </div> -->

        <!-- <figure style="width: 100%;">
            <a href="assets/DTA_real_wrold_evaluation_banner.png">
                <img width="100%" src="assets/DTA_real_wrold_evaluation_banner.png">
            </a>
        </figure> -->

        </section>

        <hr>

        <section id="results">
            <h2>DTA framework</h2>
            <hr>
            <p>
                As shown in the top picture, <b>DTA Framework</b> consists of four components: <b>Photo-Realistic
                    Rendering Engine</b>, <b>Repeated Texture Projection Function</b>, <b>Differentiable Transformation
                    Network (DTN)</b>, and the target <b>Object Detection Model</b>.
                <!-- DTA Framework is a practical approach that utilizes gradient-based optimization to find a robust physical
                adversarial pattern on legacy photo-realistic rendering, which is not always differentiable. -->
            </p>

            <h3>Photo-Realistic Rendering Engine</h3>
            <hr>
            <div class="flex-row">
                <figure style="width: 70%;">
                    <video class="centered" width="90%" controls muted loop autoplay>
                        <source src="assets/video/Photo-Realistic Rendering Demo.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                </figure>
                <div style="width: 30%;">
                    <p> <b>Photo-realistic rendering engine</b> is any software that can produce a photo-realistic image
                        which is similar to the <b>real physical world</b>. In our work, we use <a
                            href="https://carla.org/2020/12/22/release-0.9.11/">
                            Carla Simulator (ver. 0.9.11)</a> on <a href="https://www.unrealengine.com/en-US/">Unreal
                            Engine (ver. 2.4)</a> to
                        synthesize our dataset as well as to evaluate our generated texture on photo-realsitic
                        simulation setting. We modify original code to allow car's texture modification.
                        The video illustrating how the output of the rendering engine we use.
                    </p>
                </div>
            </div>

            <h3>Differentiable Transformation Network (DTN)</h3>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/dtn_architecture.png">
                    <img width="100%" src="assets/dtn_architecture.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    Our proposed <b>DTN learns the expected transformation of a rendered object when the texture is
                        changed</b> while preserving the original properties of the target object. It relies on the
                    photo-realistic image synthesized from a non-differentiable renderer to produce a differentiable
                    version of the reference image after applying the expected texture. DTN is embedded as an extention
                    to provide texture differentiablity.
                </p>
            </figure>

            <div class="flex-row">
                <div style="width: 30%;">
                    <br>
                    <br>
                    <br>
                    <p>The video ilustrate how our DTN can correctly predict the rendered image when the texture (color)
                        is changed. The network retains the original target properties such as material, light
                        reflection, and shadow from other object.
                    </p>
                </div>
                <figure style="width: 70%;">
                    <video class="centered" width="90%" controls muted loop autoplay>
                        <source src="assets/video/DTN Color Rendering Demo.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                </figure>
            </div>

            <h3>Repeated Texture Projection Function</h3>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/repeated_texture_projection_function.png">
                    <img width="75%" class="center" src="assets/repeated_texture_projection_function.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    We propose a <b>repeated pattern</b> as our final attack camouflage texture. It has several
                    benefits, such as ease of application because the texture can be used to cover the object while
                    ignoring the texture mapping. As the application, we propose a <b>Repeated Texture Projection
                        Function</b> for simply projecting the pattern with a sequence of operations by
                    <b>transformation matrix <i>M</i></b>. We use wrap mode for filling points outside boundaries, which
                    extends the output by wrapping around the opposite edge, giving a repeated texture effect.
                </p>
            </figure>

            <div class="flex-row">
                <div style="width: 30%;">
                    <br>
                    <br>
                    <p>The video illustrates how our DTN + Repeated Texture Projection Function can be used to mimic the
                        repeated pattern produced by the photo-realistic rendering engine. This gives us the
                        differentiable version of the photo-realistic renderer, allowing us to use gradient-based
                        optimization to find the optimum repeated attack texture.
                    </p>
                </div>
                <figure style="width: 70%;">
                    <video class="centered" width="90%" controls muted loop autoplay>
                        <source src="assets/video/DTN + Repeated Texture Projection Demo.webm" type="video/webm">
                        Your browser does not support the video tag.
                    </video>
                </figure>
            </div>

            <h2>Framework Procedure</h2>
            <hr>

            <h3>DTN Model Training</h3>
            <hr>

            <div class="flex-row">
                <div style="width: 30%;">
                    <br>
                    <p>
                        Before using DTA to generate the adversarial pattern, We need to train DTN with the dataset
                        generated by the photo-realistic rendering engine (see the first video on how the dataset is
                        generated). First, we select a set of random flat color textures and predefined transformations.
                        Then, we use the rendering engine to produce the photo-realistic images that will later be used
                        as reference image <i>x<sub>ref</sub></i> , expected texture <i>η<sub>exp</sub></i> , and
                        ground truth of rendered image <i>x<sub>ren</sub></i> . See the figure on the right for DTN
                        training
                        diagram.
                    </p>
                </div>
                <figure style="width: 70%;">
                    <a href="assets/dtn_training_process.png">
                        <img width="90%" class="center" src="assets/dtn_training_process.png">
                    </a>
                </figure>
            </div>

            <figure style="width: 100%;">
                <p>
                    The training histories of DTN with DenseNet
                    architecture and the prediction samples are shown below.
                </p>

                <a href="assets/dtn_densenet_4_2_histories.png">
                    <img width="100%" class="center" src="assets/dtn_densenet_4_2_histories.png">
                </a>
                <a href="assets/dtn_prediction_demo.png">
                    <img width="100%" class="center" src="assets/dtn_prediction_demo.png">
                </a>
            </figure>

            <h3>DTA Attacking Phase</h3>
            <hr>

            <div class="flex-row">
                <div style="width: 50%;">
                    <br>
                    <p>
                        In the attack phase, the goal is to minimize the original target confidence score, which
                        prevents the object detector from detecting the target object correctly. We use the
                        differentiability of the complete DTA Framework to find the best adversarial pattern
                        <i>η<sub>adv</sub></i> that minimizes the attack loss <i>L<sub>atk</sub></i> by updating the
                        <i>η<sub>adv</sub></i> based on the loss gradient. The right figure shows the training history
                        for DTA targeting EfficientDetD0 model of Toyota Camry car.
                    </p>
                </div>
                <figure style="width: 50%;">
                    <a href="assets/camry_effdetd0_attack_histories.png">
                        <img width="100%" class="center" src="assets/camry_effdetd0_attack_histories.png">
                    </a>
                </figure>
            </div>

            <figure style="width: 100%;">
                <p>
                    The sample prediction of the DTA Framework consisting of standard, random (initial), and attack
                    (final) textured cars are shown below. As we can random textured car is not sufficient to camouflage
                    the car from the object detection model.
                </p>

                <a href="assets/dta_prediction_demo.png">
                    <img width="100%" class="center" src="assets/dta_prediction_demo.png">
                </a>
            </figure>

            <!-- <figure style="width: 100%;">
                <p class="caption" style="margin-bottom: 1px;">
                    In the attack phase, the goal is to minimize the original target confidence score, which
                    prevents the object detector from detecting the target object correctly. We can use the
                    differentiability of the complete DTA Framework to find the best adversarial pattern
                    <i>η<sub>adv</sub></i> that minimizes the attack loss <i>L<sub>atk</sub></i> by updating the
                    <i>η<sub>adv</sub></i> based on the loss gradient.
                </p>
            </figure> -->

            <h2>Evaluation Results</h2>
            <hr>
            <p>
                We compare our adversarial camouflage with a random pattern and previous works on 3D physical attacks:
                <a href="https://openreview.net/forum?id=SJgEl3A5tm">CAMOU</a>,
                <a href="https://arxiv.org/abs/2007.16118">ER</a>,
                <a
                    href="https://openaccess.thecvf.com/content_CVPR_2020/html/Huang_Universal_Physical_Camouflage_Attacks_on_Object_Detectors_CVPR_2020_paper.html">UPC</a>,
                and
                <a
                    href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Dual_Attention_Suppression_Attack_Generate_Adversarial_Camouflage_in_Physical_World_CVPR_2021_paper.html">DAS</a>
                We closely follow the approach to replicate the original papers, but we rebuild the environment and
                target models based on our evaluation setup (see supplementary material for the details). However,
                <a
                    href="https://openaccess.thecvf.com/content_CVPR_2020/html/Huang_Universal_Physical_Camouflage_Attacks_on_Object_Detectors_CVPR_2020_paper.html">UPC</a>
                and
                <a
                    href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Dual_Attention_Suppression_Attack_Generate_Adversarial_Camouflage_in_Physical_World_CVPR_2021_paper.html">DAS</a>
                have different settings to recreate in our environment; thus, we only evaluate them on the
                transferability experiment.
            </p>

            <h3>Photo-Realistic Simulation Evaluation</h3>
            <hr>
            <p style="text-align: center;">[Click the link to show sample demo videos]</p>
            <h4><a href="evaluation/effdetd0_5d_15p.html">Target: EfficientDetD0 | Camera: 5m Distance, 15° Pitch, 360°
                    Rotation</a></h4>
            <h4><a href="evaluation/effdetd0_10d_15p.html">Target: EfficientDetD0 | Camera: 10m Distance, 15° Pitch, 360°
                    Rotation</a></h4>
            <h4><a href="evaluation/effdetd0_10d_30p.html">Target: EfficientDetD0 | Camera: 10m Distance, 30°
                    Pitch, 360°
                    Rotation</a></h4>
            <h4><a href="evaluation/effdetd0_15d_30p.html">Target: EfficientDetD0 | Camera: 15m Distance, 30°
                    Pitch, 360°
                    Rotation</a></h4>

            <h3>Transferability Evaluation</h3>
            <hr>
            <p style="text-align: center;">[Click the link to show sample demo videos]</p>
            <h4><a href="evaluation/ssd_5d_0p.html">Target: SSD | Camera: 5m Distance, 0° Pitch, 360°
                    Rotation</a></h4>
            <h4><a href="evaluation/fasterrcnn_10d_15p.html">Target: Faster-RCNN | Camera: 10m Distance, 15° Pitch,
                    360°
                    Rotation</a></h4>
            <h4><a href="evaluation/maskrcnn_10d_30p.html">Target: Mask-RCNN | Camera: 10m Distance, 30° Pitch,
                    360°
                    Rotation</a></h4>

            <h3>Real-World Evaluation</h3>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/RealWorld4x4.png">
                    <img width="100%" src="assets/RealWorld4x4.png">
                </a>
                <p style="margin-bottom: 1px;">
                    
                </p>
            </figure>
            Videos are available soon....

        </section>

        <section id="bibtex">
            <h2>Citation</h2>
            <hr>
            <pre><code>
                Available soon....
                @article{...,
                    author = {...},
                    title = "{...}",
                    journal = {...},
                    year = {...}
                }
            </code></pre>
        </section>

        <br />
        <section id="paper">
            <h2>Paper</h2>
            <hr>
            <div class="flex-row">
                <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                    <a href="assets/paper_preview.png"><img class="screenshot" src="assets/paper_preview.png"></a>
                </div>
                <div style="width: 50%">
                    <p><b>DTA: Physical Camouflage Attacks using Differentiable Transformation Network</b></p>
                    <p>
                        Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma Larasati,
                        Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, Howon Kim
                    </p>
                    <div><span class="material-icons"> description </span><a href="#avs" rel="modal:open"> Paper</a>
                    </div>
                    <div><span class="material-icons"> description </span><a href="#avs" rel="modal:open">
                            Supplementary</a></div>
                    <div><span class="material-icons"> description </span><a href="#avs" rel="modal:open"> arXiv</a></div>
                    <div><span class="material-icons"> insert_comment </span><a href="#avs" rel="modal:open"> BibTeX</a>
                    </div>
                </div>
            </div>
        </section>


        <!-- <section id="teaser-videos">
            <div class="flex-row">
                <figure style="width: 70%;">
                    <video class="centered" width="90%" controls muted loop autoplay>
                        <source src="assets/video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
                <div style="width: 30%;">
                    <br><br>
                    <p>Video illustrating our training progress, scene editing examples and automatic LOD. All examples
                        enabled by our <strong>explicit</strong> decomposition
                        into a triangle mesh, PBR materials and HDR environment light, directly compatible with
                        traditional graphics engines.
                        Feel free to download the <a href="assets/video.mp4">video</a>, native resolution: 1024x1024
                        pixels.
                    </p>
                </div>
            </div>
        </section> -->
        <!-- 
        <section id="results">

            <h2>3D model reconstruction and intrinsic decomposition from images</h2>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/materials.JPG">
                    <img width="100%" src="assets/materials.JPG">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    Our reconstruction from 100 images. We reconstruct a triangle mesh, PBR materials stored in 2D
                    textures, and an HDR environment map.
                    Materials scene from the <a href="https://github.com/bmild/nerf">NeRF synthetic dataset</a>.
                </p>
            </figure>

            <h2>Scene manipulation with the reconstructed models</h2>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/teaser.JPG">
                    <img width="100%" src="assets/teaser.JPG">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    We reconstruct a triangular mesh with unknown topology, spatially-varying materials, and lighting
                    from a set of multi-view
                    images. We show examples of scene manipulation using off-the-shelf modeling tools, enabled by our
                    reconstructed 3D model.
                    Dataset from <a href="https://markboss.me/publication/2021-nerd/">NeRD: Neural Reflectance
                        Decomposition from Image Collections</a>.
                </p>
            </figure>

            <h2>All-frequency environment lighting</h2>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/splitsum.JPG">
                    <img width="100%" src="assets/splitsum.JPG">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    Environment lighting approximated with Spherical Gaussians using 128 lobes vs. Split Sum. The
                    training set consists of 256
                    path traced images with Monte Carlo sampled environment lighting using a high resolution HDR probe.
                    We assume known geometry and
                    optimize materials and lighting using identical settings for both methods. Reported image metrics
                    are the arithmetic mean of the 16 (novel)
                    views in the test set. Note that the split sum approximation is able to capture high frequency
                    lighting.
                    Probe from <a href="https://polyhaven.com/"> Poly Haven</a>.
                </p>
            </figure>

        </section>

        <section id="bibtex">
            <h2>Citation</h2>
            <hr>
            <pre><code>
        @article{munkberg2021nvdiffrec,
            author = {Jacob Munkberg and Jon Hasselgren and Tianchang Shen and Jun Gao 
                    and Wenzheng Chen and Alex Evans and Thomas Mueller and Sanja Fidler},
            title = "{Extracting Triangular 3D Models, Materials, and Lighting From Images}",
            journal = {arXiv:2111.12503},
            year = {2021}
        }
</code></pre>
        </section>

        <br />
        <section id="paper">
            <h2>Paper</h2>
            <hr>
            <div class="flex-row">
                <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                    <a href="assets/paper.pdf"><img class="screenshot" src="assets/paper_preview.JPG"></a>
                </div>
                <div style="width: 50%">
                    <p><b>Extracting Triangular 3D Models, Materials, and Lighting From Images</b></p>
                    <p>Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas
                        Müller, Sanja Fidler</p>
                    <div><span class="material-icons"> description </span><a href="assets/paper.pdf"> Preprint</a></div>
                    <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2111.12503">
                            arXiv version</a></div>
                    <div><span class="material-icons"> description </span><a href="assets/video.mp4"> Video</a></div>
                    <div><span class="material-icons"> insert_comment </span><a href="assets/bib.txt"> BibTeX</a></div>
                </div>
            </div>
        </section> -->

    </div>

    <script src="script.js"></script>
</body>

</html>