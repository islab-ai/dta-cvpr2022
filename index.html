<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
    google.load("jquery", "1.3.2");
</script>

<style type="text/css">

</style>


<div class="topnav" id="myTopnav">
    <a href="http://infosec.pusan.ac.kr/"><img width="100%" src="assets/infosec_logo.png"></a>
    <a href="https://www.smartm2m.co.kr/"><img width="100%" src="assets/smartm2m_blk_logo.png"></a>
    <a href="https://add.re.kr/"><img width="100%" src="assets/add_logo.png"></a>
</div>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>

<head>
    <title>DTA: Physical Camouflage Attacks using Differentiable Transformation Network</title>
    <meta property="og:description"
        content="DTA: Physical Camouflage Attacks using Differentiable Transformation Network" />
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

    <head>
        <link rel="stylesheet" href="style.css">
    </head>

</head>


<body>
    <div class="container">
        <div class="paper-title">
            <h1>DTA: Physical Camouflage Attacks using Differentiable Transformation Network</h1>
            <h2>CVPR 2022 - Accepted Paper</h2>
        </div>


        <div id="authors">
            <div class="author-row">
                <div class="col-3 text-center"><a href="https://www.linkedin.com/in/naufal-suryanto/">Naufal
                        Suryanto</a><sup>1</sup></div>
                <div class="col-3 text-center"><a href="#">Yongsu Kim</a><sup>1,2</sup></div>
                <div class="col-3 text-center"><a href="#">Hyoeun Kang</a><sup>1</sup></div>
                <div class="col-3 text-center"><a href="#">Harashta Tatimma Larasati</a><sup>1</sup></div>
                <div class="col-3 text-center"><a href="#">Youngyeo Yun</a><sup>1</sup></div>
                <div class="col-3 text-center"><a href="#">Thi-Thu-Huong Le</a><sup>1</sup></div>
                <div class="col-3 text-center"><a href="#">Hunmin Yang</a><sup>3</sup></div>
                <div class="col-3 text-center"><a href="#">Se-Yoon Oh</a><sup>3</sup></div>
                <div class="col-3 text-center"><a href="#">Howon Kim</a><sup>1,2</sup></div>
            </div>

            <div class="affil-row">
                <div class="col-3 text-center"><sup>1</sup>Pusan National University</a></div>
                <div class="col-3 text-center"><sup>2</sup>SmartM2M</div>
                <div class="col-3 text-center"><sup>3</sup>Agency for Defense Development</div>
            </div>
            <!-- <div class="affil-row">
            <div class="venue text-center"><b></b></div>
        </div> -->

            <div style="clear: both">
                <div class="paper-btn-parent">
                    <a class="supp-btn" href="#">
                        <span class="material-icons"> description </span>
                        Paper
                    </a>
                    <a class="supp-btn" href="#">
                        <span class="material-icons"> description </span>
                        BibTeX
                    </a>
                </div>
            </div>
        </div>

        <section id="teaser">
            <figure style="width: 100%;">
                <a href="assets/DTA_attack_pipeline.png">
                    <img width="100%" src="assets/DTA_attack_pipeline.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    <b>Differentiable Transformation Attack (DTA) </b> is our proposed framework for generating a robust
                    physical adversarial pattern on a target object to camouflage it against object detection models
                    under a wide range of transformations.
                    Our framework uses legacy photo-realistic renderers for simulating physical-world transformations
                    and employs our novel <b>Differentiable Transformation Network (DTN)</b> to enable the texture
                    differentiability. Our framework produces a robust adversarial pattern that is applicable and
                    transferable even in the real-world.
                    <!-- DTN is trained to learn the expected transformation of a rendered object when the texture is
                    changed while retaining the target object's original properties. -->
                </p>
            </figure>
        </section>

        <section id="abstract" />
        <h2>Abstract</h2>
        <hr>
        <p>
            To perform adversarial attacks in the physical world, many studies have proposed adversarial camouflage, a
            method to hide a target object by applying camouflage patterns on 3D object surfaces.
            For obtaining optimal physical adversarial camouflage, previous studies have utilized the so-called neural
            renderer, as it supports differentiability. However, existing neural renderers cannot fully represent
            various real-world transformations due to a lack of control of scene parameters compared to the legacy
            photo-realistic renderers. In this paper, we propose the Differentiable Transformation Attack (DTA), a
            framework for generating a robust physical adversarial pattern on a target object to camouflage it against
            object detection models with a wide range of transformations. It utilizes our novel Differentiable
            Transformation Network (DTN), which learns the expected transformation of a rendered object when the texture
            is changed while preserving the original properties of the target object. Using our attack framework, an
            adversary can gain both the advantages of the legacy photo-realistic renderers including various
            physical-world transformations and the benefit of white-box access by offering differentiability. Our
            experiments show that our camouflaged 3D vehicles can successfully evade state-of-the-art object
            detection models in the photo-realistic environment (i.e., <a href="https://carla.org/">CARLA</a> on <a
                href="https://www.unrealengine.com/en-US/">Unreal Engine</a>). Furthermore, our
            demonstration on a scaled Tesla Model 3 proves the applicability and transferability of our method to the
            real world.
        </p>

        <div class="flex-row">
            <figure style="width: 50%;">
                <a href="assets/video/tesla_normal_detect.webp">
                    <img width="98%" src="assets/video/tesla_normal_detect.webp">
                </a>
            </figure>
            <figure style="width: 50%;">
                <a href="assets/video/tesla_attack_detect.webp">
                    <img width="98%" src="assets/video/tesla_attack_detect.webp">
                </a>
            </figure>
        </div>

        <!-- <figure style="width: 100%;">
            <a href="assets/DTA_real_wrold_evaluation_banner.png">
                <img width="100%" src="assets/DTA_real_wrold_evaluation_banner.png">
            </a>
        </figure> -->

        </section>

        <hr>

        <section id="results">

            <h2>Photo Realistic-Rendering Engine</h2>
            <hr>
            <div class="flex-row">
                <figure style="width: 70%;">
                    <video class="centered" width="90%" controls muted loop autoplay>
                        <source src="assets/video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
                <div style="width: 30%;">
                    <p>Photo-realistic rendering engine is any software that can produce a photo-realistic image which
                        is similar to the real physical world. In our work, we use <a
                            href="https://carla.org/2020/12/22/release-0.9.11/">
                            Carla Simulator (ver. 0.9.11)</a> on <a href="https://www.unrealengine.com/en-US/">Unreal
                            Engine (ver. 2.4)</a> to
                        synthesize our dataset as well as to evaluate our generated texture on photo-realsitic
                        simulation setting. We modify original code to allow car's texture modification.
                        The video illustrating how the output of the rendering engine we use.
                    </p>
                </div>
            </div>

            <h2>Differentiable Transformation Network (DTN)</h2>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/dtn_architecture.png">
                    <img width="100%" src="assets/dtn_architecture.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    Our proposed DTN learns the expected transformation of a rendered object when the texture is changed
                    while preserving the original properties of the target object. It relies on the photo-realistic
                    image synthesized from a non-differentiable renderer to produce a differentiable version of the
                    reference image after applying the expected texture. DTN is embedded as an extention to provide
                    texture differentiablity.
                </p>
            </figure>

            <div class="flex-row">
                <div style="width: 30%;">
                    <br>
                    <br>
                    <p>The video ilustrate how our DTN can correctly predict the rendered image when the texture (color)
                        is changed. The network retains the original target properties such as material, light
                        reflection, and shadow from other object.
                    </p>
                </div>
                <figure style="width: 70%;">
                    <video class="centered" width="90%" controls muted loop autoplay>
                        <source src="assets/video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
            </div>

            <h2>Repeated Texture Projection Function</h2>
            <hr>

        </section>


        <!-- <section id="teaser-videos">
            <div class="flex-row">
                <figure style="width: 70%;">
                    <video class="centered" width="90%" controls muted loop autoplay>
                        <source src="assets/video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
                <div style="width: 30%;">
                    <br><br>
                    <p>Video illustrating our training progress, scene editing examples and automatic LOD. All examples
                        enabled by our <strong>explicit</strong> decomposition
                        into a triangle mesh, PBR materials and HDR environment light, directly compatible with
                        traditional graphics engines.
                        Feel free to download the <a href="assets/video.mp4">video</a>, native resolution: 1024x1024
                        pixels.
                    </p>
                </div>
            </div>
        </section> -->
        <!-- 
        <section id="results">

            <h2>3D model reconstruction and intrinsic decomposition from images</h2>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/materials.JPG">
                    <img width="100%" src="assets/materials.JPG">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    Our reconstruction from 100 images. We reconstruct a triangle mesh, PBR materials stored in 2D
                    textures, and an HDR environment map.
                    Materials scene from the <a href="https://github.com/bmild/nerf">NeRF synthetic dataset</a>.
                </p>
            </figure>

            <h2>Scene manipulation with the reconstructed models</h2>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/teaser.JPG">
                    <img width="100%" src="assets/teaser.JPG">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    We reconstruct a triangular mesh with unknown topology, spatially-varying materials, and lighting
                    from a set of multi-view
                    images. We show examples of scene manipulation using off-the-shelf modeling tools, enabled by our
                    reconstructed 3D model.
                    Dataset from <a href="https://markboss.me/publication/2021-nerd/">NeRD: Neural Reflectance
                        Decomposition from Image Collections</a>.
                </p>
            </figure>

            <h2>All-frequency environment lighting</h2>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/splitsum.JPG">
                    <img width="100%" src="assets/splitsum.JPG">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    Environment lighting approximated with Spherical Gaussians using 128 lobes vs. Split Sum. The
                    training set consists of 256
                    path traced images with Monte Carlo sampled environment lighting using a high resolution HDR probe.
                    We assume known geometry and
                    optimize materials and lighting using identical settings for both methods. Reported image metrics
                    are the arithmetic mean of the 16 (novel)
                    views in the test set. Note that the split sum approximation is able to capture high frequency
                    lighting.
                    Probe from <a href="https://polyhaven.com/"> Poly Haven</a>.
                </p>
            </figure>

        </section>

        <section id="bibtex">
            <h2>Citation</h2>
            <hr>
            <pre><code>
        @article{munkberg2021nvdiffrec,
            author = {Jacob Munkberg and Jon Hasselgren and Tianchang Shen and Jun Gao 
                    and Wenzheng Chen and Alex Evans and Thomas Mueller and Sanja Fidler},
            title = "{Extracting Triangular 3D Models, Materials, and Lighting From Images}",
            journal = {arXiv:2111.12503},
            year = {2021}
        }
</code></pre>
        </section>

        <br />
        <section id="paper">
            <h2>Paper</h2>
            <hr>
            <div class="flex-row">
                <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                    <a href="assets/paper.pdf"><img class="screenshot" src="assets/paper_preview.JPG"></a>
                </div>
                <div style="width: 50%">
                    <p><b>Extracting Triangular 3D Models, Materials, and Lighting From Images</b></p>
                    <p>Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas
                        Müller, Sanja Fidler</p>
                    <div><span class="material-icons"> description </span><a href="assets/paper.pdf"> Preprint</a></div>
                    <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2111.12503">
                            arXiv version</a></div>
                    <div><span class="material-icons"> description </span><a href="assets/video.mp4"> Video</a></div>
                    <div><span class="material-icons"> insert_comment </span><a href="assets/bib.txt"> BibTeX</a></div>
                </div>
            </div>
        </section> -->

    </div>
</body>

</html>